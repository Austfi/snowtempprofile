{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected station: Basin\n",
      "URL being fetched:\n",
      " https://stations.avalanche.state.co.us/tabular.php?title=A-Basin+SA-Pali+(A-BasinSA)+11920+ft&st=CAABP&date=2025-01-20+24&unit=e&area=caic&range=240\n",
      "Plot URL being fetched:\n",
      " https://stations.avalanche.state.co.us/hplot.php?title=A-Basin+SA-Pali+(A-BasinSA)+11920+ft&st=CAABP&date=2025-01-20+24&unit=e&area=caic&range=240\n",
      "Scraped 241 data rows for station Basin.\n",
      "Wrote 241 lines to basin_collect.txt.\n",
      "\n",
      "Selected station: Beck\n",
      "URL being fetched:\n",
      " https://stations.avalanche.state.co.us/tabular.php?title=Senator+Beck+%28CSAS%29+12186+ft&st=CASBK&date=2025-01-20+24&unit=e&area=caic&range=240\n",
      "Plot URL being fetched:\n",
      " https://stations.avalanche.state.co.us/hplot.php?title=Senator+Beck+%28CSAS%29+12186+ft&st=CASBK&date=2025-01-20+24&unit=e&area=caic&range=240\n",
      "Scraped 241 data rows for station Beck.\n",
      "Wrote 241 lines to beck_collect.txt.\n",
      "\n",
      "Selected station: Boss\n",
      "URL being fetched:\n",
      " https://stations.avalanche.state.co.us/tabular.php?title=Boss+Basin+%28USGS%29+11259+ft&st=USBBN&date=2025-01-20+24&unit=e&area=caic&range=240\n",
      "Plot URL being fetched:\n",
      " https://stations.avalanche.state.co.us/hplot.php?title=Boss+Basin+%28USGS%29+11259+ft&st=USBBN&date=2025-01-20+24&unit=e&area=caic&range=240\n",
      "Scraped 240 data rows for station Boss.\n",
      "Wrote 240 lines to boss_collect.txt.\n",
      "\n",
      "Overwrote beck_collect.txt with USGS Tsurf(C) appended. Backup saved as beck_collect.txt.backup.\n",
      "Data collection complete.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "data_collect.py\n",
    "\n",
    "This script:\n",
    "  1. Scrapes station data from CAIC (Basin, Beck, Boss) and saves them as text files.\n",
    "  2. For the Beck station, fetches USGS NWIS surface temperature data directly from the URL,\n",
    "     builds an interpolation function relative to the earliest time in beck_collect.txt,\n",
    "     and overwrites beck_collect.txt with an extra column containing the USGS data.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "import numpy as np\n",
    "import shutil\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "###############################################################################\n",
    "# SECTION 1: CAIC DATA SCRAPING\n",
    "###############################################################################\n",
    "\n",
    "# Station options with parameters for URL construction and output formatting.\n",
    "station_options = {\n",
    "    \"Basin\": {\n",
    "        \"station_title\": \"A-Basin+SA-Pali+(A-BasinSA)+11920+ft\",\n",
    "        \"station_code\": \"CAABP\",\n",
    "        \"expected_numeric_cols\": 13,\n",
    "        \"remove_index\": None\n",
    "    },\n",
    "    \"Beck\": {\n",
    "        \"station_title\": \"Senator+Beck+%28CSAS%29+12186+ft\",\n",
    "        \"station_code\": \"CASBK\",\n",
    "        \"expected_numeric_cols\": 14,\n",
    "        \"remove_index\": 8\n",
    "    },\n",
    "    \"Boss\": {\n",
    "        \"station_title\": \"Boss+Basin+%28USGS%29+11259+ft\",\n",
    "        \"station_code\": \"USBBN\",\n",
    "        \"expected_numeric_cols\": 14,\n",
    "        \"remove_index\": 7\n",
    "    }\n",
    "}\n",
    "\n",
    "def scrape_station_data(end_date_str=\"2025-03-01+24\", hours_range=72,\n",
    "                        station_title=\"A-Basin+SA-Pali+(A-BasinSA)+11920+ft\",\n",
    "                        station_code=\"CAABP\",\n",
    "                        expected_numeric_cols=13,\n",
    "                        remove_index=None):\n",
    "    \"\"\"\n",
    "    Scrape CAIC station data for the given time range.\n",
    "    Returns a list of strings. Each line is formatted as:\n",
    "      YYYY Mon DD HH:MM  Temp MxTp MnTp Dew(F) RH Spd Dir Gst SWIN SWOUT LWIN LWOUT NET\n",
    "    (17 columns total: 4 date/time columns plus 13 numeric values).\n",
    "    \"\"\"\n",
    "    url = (\n",
    "        \"https://stations.avalanche.state.co.us/tabular.php\"\n",
    "        f\"?title={station_title}\"\n",
    "        f\"&st={station_code}\"\n",
    "        f\"&date={end_date_str}\"\n",
    "        \"&unit=e\"\n",
    "        \"&area=caic\"\n",
    "        f\"&range={hours_range}\"\n",
    "    )\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(f\"Request failed with status {response.status_code}\")\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    pre_tag = soup.find(\"pre\")\n",
    "    if not pre_tag:\n",
    "        raise ValueError(\"Could not find <pre> block with data.\")\n",
    "\n",
    "    lines = pre_tag.text.strip().split(\"\\n\")\n",
    "\n",
    "    def convert_to_24h(timestr_12h, ampm):\n",
    "        \"\"\"Convert a 12-hour time string (with am/pm) into 24-hour format.\"\"\"\n",
    "        hour_12, minute = timestr_12h.split(\":\")\n",
    "        hour_12 = int(hour_12)\n",
    "        minute = int(minute)\n",
    "        if hour_12 == 12 and ampm.lower() == \"am\":\n",
    "            return \"00:00\"\n",
    "        if hour_12 == 12 and ampm.lower() == \"pm\":\n",
    "            return f\"12:{minute:02d}\"\n",
    "        if ampm.lower() == \"pm\":\n",
    "            return f\"{hour_12 + 12:02d}:{minute:02d}\"\n",
    "        return f\"{hour_12:02d}:{minute:02d}\"\n",
    "\n",
    "    data_lines = []\n",
    "    for row in lines:\n",
    "        row = row.strip()\n",
    "        # Skip headers and blank lines.\n",
    "        if not row or row.startswith(\"Date\") or row.startswith(station_title.replace(\"+\", \" \")):\n",
    "            continue\n",
    "        parts = row.split()\n",
    "        if len(parts) < 5 + expected_numeric_cols:\n",
    "            continue\n",
    "\n",
    "        # Determine if an AM/PM token is present.\n",
    "        possible_ampm = parts[4].lower()\n",
    "        if possible_ampm in (\"am\", \"pm\"):\n",
    "            time_12h = parts[3]\n",
    "            ampm = parts[4]\n",
    "            offset = 5\n",
    "        else:\n",
    "            time_12h = parts[3]\n",
    "            ampm = None\n",
    "            offset = 4\n",
    "\n",
    "        year_str, month_str, day_str = parts[0], parts[1], parts[2]\n",
    "        time_24 = convert_to_24h(time_12h, ampm) if ampm else time_12h\n",
    "\n",
    "        # Extract numeric tokens.\n",
    "        numeric_parts = parts[offset:]\n",
    "        if len(numeric_parts) < expected_numeric_cols:\n",
    "            continue\n",
    "        numeric_cols = numeric_parts[:expected_numeric_cols]\n",
    "\n",
    "        if remove_index is not None:\n",
    "            del numeric_cols[remove_index]\n",
    "\n",
    "        out_line = f\"{year_str} {month_str} {day_str} {time_24}\"\n",
    "        for val in numeric_cols:\n",
    "            out_line += f\"  {val}\"\n",
    "        data_lines.append(out_line)\n",
    "\n",
    "    return data_lines\n",
    "\n",
    "def build_caic_url(end_date_str=\"2025-03-01+24\", hours_range=72,\n",
    "                   station_title=\"A-Basin+SA-Pali+(A-BasinSA)+11920+ft\",\n",
    "                   station_code=\"CAABP\"):\n",
    "    \"\"\"Build the CAIC URL for tabular data.\"\"\"\n",
    "    return (\n",
    "        \"https://stations.avalanche.state.co.us/tabular.php\"\n",
    "        f\"?title={station_title}\"\n",
    "        f\"&st={station_code}\"\n",
    "        f\"&date={end_date_str}\"\n",
    "        \"&unit=e\"\n",
    "        \"&area=caic\"\n",
    "        f\"&range={hours_range}\"\n",
    "    )\n",
    "\n",
    "def build_caic_plot_url(end_date_str=\"2025-03-01+24\", hours_range=72,\n",
    "                        station_title=\"A-Basin+SA-Pali+(A-BasinSA)+11920+ft\",\n",
    "                        station_code=\"CAABP\"):\n",
    "    \"\"\"Build the CAIC URL for plotting data.\"\"\"\n",
    "    return (\n",
    "        \"https://stations.avalanche.state.co.us/hplot.php\"\n",
    "        f\"?title={station_title}\"\n",
    "        f\"&st={station_code}\"\n",
    "        f\"&date={end_date_str}\"\n",
    "        \"&unit=e\"\n",
    "        \"&area=caic\"\n",
    "        f\"&range={hours_range}\"\n",
    "    )\n",
    "\n",
    "###############################################################################\n",
    "# SECTION 2: USGS NWIS MERGE FUNCTIONS (for Senator Beck)\n",
    "###############################################################################\n",
    "\n",
    "def read_earliest_time_from_beck(beck_file):\n",
    "    \"\"\"\n",
    "    Reads beck_collect.txt and returns the earliest timestamp (as datetime)\n",
    "    from the first valid data row.\n",
    "    \"\"\"\n",
    "    month_map = {\n",
    "        'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
    "        'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
    "    }\n",
    "    earliest_time = None\n",
    "    with open(beck_file, 'r') as fin:\n",
    "        for line in fin:\n",
    "            line_str = line.strip()\n",
    "            if not line_str:\n",
    "                continue\n",
    "            parts = line_str.split()\n",
    "            if len(parts) < 17:\n",
    "                continue\n",
    "            try:\n",
    "                year_str, month_str, day_str = parts[0], parts[1], parts[2]\n",
    "                hhmm_str = parts[3]\n",
    "                year = int(year_str)\n",
    "                month = month_map[month_str]\n",
    "                day = int(day_str)\n",
    "                hh_part, mm_part = hhmm_str.split(':')\n",
    "                hour_24 = int(hh_part)\n",
    "                minute = int(mm_part)\n",
    "                if hour_24 == 24:\n",
    "                    hour_24 = 0\n",
    "                    dt_obj = datetime.datetime(year, month, day) + datetime.timedelta(days=1)\n",
    "                else:\n",
    "                    dt_obj = datetime.datetime(year, month, day, hour_24, minute)\n",
    "                earliest_time = dt_obj\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "    if earliest_time is None:\n",
    "        raise ValueError(\"No valid data lines found in beck_collect.txt for earliest time.\")\n",
    "    return earliest_time\n",
    "\n",
    "def fetch_usgs_data(usgs_url, earliest_time):\n",
    "    \"\"\"\n",
    "    Fetches USGS NWIS data from the provided URL (in rdb format), parses it,\n",
    "    and returns two numpy arrays: times (in hours since earliest_time) and temperatures (°C).\n",
    "    \n",
    "    Expected format (tab-delimited, with comment lines starting with '#' or header 'agency_cd'):\n",
    "      USGS   375429107433201   2024-04-03 21:00   MDT   -11.0   P\n",
    "    \"\"\"\n",
    "    response = requests.get(usgs_url)\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(\"Failed to fetch USGS data; status: \" + str(response.status_code))\n",
    "    text = response.text\n",
    "\n",
    "    times_list = []\n",
    "    temps_list = []\n",
    "    for line in text.splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith('#') or line.startswith('agency_cd'):\n",
    "            continue\n",
    "        parts = line.split('\\t')\n",
    "        if len(parts) < 6:\n",
    "            continue\n",
    "        date_str = parts[2]   # e.g., \"2024-04-03 21:00\"\n",
    "        temp_str = parts[4]   # e.g., \"-11.0\"\n",
    "        try:\n",
    "            # Try parsing with \"%Y-%m-%d %H:%M\" and fall back to \"%Y-%m-%d %H:%M:%S\" if needed.\n",
    "            try:\n",
    "                dt_obj = datetime.datetime.strptime(date_str, \"%Y-%m-%d %H:%M\")\n",
    "            except ValueError:\n",
    "                dt_obj = datetime.datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "        except ValueError:\n",
    "            continue\n",
    "        try:\n",
    "            T_c = float(temp_str)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        dt_diff = dt_obj - earliest_time\n",
    "        dt_hours = dt_diff.total_seconds() / 3600.0\n",
    "        times_list.append(dt_hours)\n",
    "        temps_list.append(T_c)\n",
    "    if len(times_list) == 0:\n",
    "        raise ValueError(\"No valid USGS data found from URL.\")\n",
    "    times_arr = np.array(times_list)\n",
    "    temps_arr = np.array(temps_list)\n",
    "    idx_sort = np.argsort(times_arr)\n",
    "    return times_arr[idx_sort], temps_arr[idx_sort]\n",
    "\n",
    "def overwrite_beck_with_usgs(beck_file, usgs_temp_interp, earliest_time):\n",
    "    \"\"\"\n",
    "    Reads beck_collect.txt line by line, re-parses the timestamp for each data row,\n",
    "    computes the USGS temperature using the interpolation function, and appends\n",
    "    the temperature (°C) as a new column. Overwrites beck_collect.txt in place,\n",
    "    keeping a backup as beck_collect.txt.backup.\n",
    "    \"\"\"\n",
    "    backup_file = beck_file + \".backup\"\n",
    "    shutil.copy2(beck_file, backup_file)\n",
    "\n",
    "    month_map = {\n",
    "        'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4, 'May': 5, 'Jun': 6,\n",
    "        'Jul': 7, 'Aug': 8, 'Sep': 9, 'Oct': 10, 'Nov': 11, 'Dec': 12\n",
    "    }\n",
    "\n",
    "    with open(backup_file, 'r') as fin, open(beck_file, 'w') as fout:\n",
    "        for line in fin:\n",
    "            line_str = line.strip()\n",
    "            if not line_str:\n",
    "                fout.write(line)\n",
    "                continue\n",
    "            parts = line_str.split()\n",
    "            if len(parts) < 17:\n",
    "                fout.write(line)\n",
    "                continue\n",
    "            try:\n",
    "                year_str, month_str, day_str = parts[0], parts[1], parts[2]\n",
    "                hhmm_str = parts[3]\n",
    "                year = int(year_str)\n",
    "                month = month_map[month_str]\n",
    "                day = int(day_str)\n",
    "                hh_part, mm_part = hhmm_str.split(':')\n",
    "                hour_24 = int(hh_part)\n",
    "                minute = int(mm_part)\n",
    "                if hour_24 == 24:\n",
    "                    hour_24 = 0\n",
    "                    dt_obj = datetime.datetime(year, month, day) + datetime.timedelta(days=1)\n",
    "                else:\n",
    "                    dt_obj = datetime.datetime(year, month, day, hour_24, minute)\n",
    "                dt_hours = (dt_obj - earliest_time).total_seconds() / 3600.0\n",
    "                usgs_T_c = usgs_temp_interp(dt_hours)\n",
    "                new_line = f\"{line_str}  {usgs_T_c:.2f}\\n\"\n",
    "                fout.write(new_line)\n",
    "            except Exception:\n",
    "                fout.write(line)\n",
    "    print(f\"Overwrote {beck_file} with USGS Tsurf(C) appended. Backup saved as {backup_file}.\")\n",
    "\n",
    "###############################################################################\n",
    "# SECTION 3: MAIN SCRIPT FLOW\n",
    "###############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Step 1: Scrape CAIC data for all stations ---\n",
    "    end_date_str = \"2025-01-20+24\"  # e.g., midnight leading into the next day\n",
    "    hours_range = 24 * 10           # number of hours of data to fetch\n",
    "\n",
    "    for station_key, station_info in station_options.items():\n",
    "        print(\"Selected station:\", station_key)\n",
    "        caic_url = build_caic_url(\n",
    "            end_date_str,\n",
    "            hours_range,\n",
    "            station_title=station_info[\"station_title\"],\n",
    "            station_code=station_info[\"station_code\"]\n",
    "        )\n",
    "        caic_plot_url = build_caic_plot_url(\n",
    "            end_date_str,\n",
    "            hours_range,\n",
    "            station_title=station_info[\"station_title\"],\n",
    "            station_code=station_info[\"station_code\"]\n",
    "        )\n",
    "        print(\"URL being fetched:\\n\", caic_url)\n",
    "        print(\"Plot URL being fetched:\\n\", caic_plot_url)\n",
    "\n",
    "        data_lines = scrape_station_data(\n",
    "            end_date_str,\n",
    "            hours_range,\n",
    "            station_title=station_info[\"station_title\"],\n",
    "            station_code=station_info[\"station_code\"],\n",
    "            expected_numeric_cols=station_info[\"expected_numeric_cols\"],\n",
    "            remove_index=station_info[\"remove_index\"]\n",
    "        )\n",
    "        print(f\"Scraped {len(data_lines)} data rows for station {station_key}.\")\n",
    "\n",
    "        out_filename = f\"{station_key.lower()}_collect.txt\"\n",
    "        with open(out_filename, \"w\") as f:\n",
    "            for line in data_lines:\n",
    "                f.write(line + \"\\n\")\n",
    "        print(f\"Wrote {len(data_lines)} lines to {out_filename}.\\n\")\n",
    "\n",
    "    # --- Step 2: Merge USGS NWIS data into beck_collect.txt ---\n",
    "    # This step is only for the Beck station.\n",
    "    beck_file = \"beck_collect.txt\"\n",
    "    try:\n",
    "        # Get the earliest timestamp from beck_collect.txt\n",
    "        earliest_time = read_earliest_time_from_beck(beck_file)\n",
    "        # Use the provided USGS NWIS URL\n",
    "        usgs_url = (\"https://nwis.waterservices.usgs.gov/nwis/iv/?sites=375429107433201\"\n",
    "                    \"&agencyCd=USGS&startDT=2024-04-03T20:15:56.246-06:00\"\n",
    "                    \"&endDT=2025-04-03T20:15:56.246-06:00&parameterCd=72405&format=rdb\")\n",
    "        times_usgs, temps_usgsC = fetch_usgs_data(usgs_url, earliest_time)\n",
    "        usgs_temp_interp = interp1d(times_usgs, temps_usgsC, kind='linear', fill_value='extrapolate')\n",
    "        # Overwrite beck_collect.txt with the merged USGS surface temperature column.\n",
    "        overwrite_beck_with_usgs(beck_file, usgs_temp_interp, earliest_time)\n",
    "    except Exception as e:\n",
    "        print(\"Skipping USGS merging for Beck. Reason:\", e)\n",
    "\n",
    "    print(\"Data collection complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ATO_4850_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
